{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import json\n",
    "import re\n",
    "import utils\n",
    "import param_V_CNN_C_LSTM as pr\n",
    "\n",
    "SEED = 12345\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.compat.v1.random.set_random_seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "def preprocess(dataset):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scalers = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scalers.fit_transform(dataset)\n",
    "    return scaled, scalers\n",
    "\n",
    "\n",
    "def extract_VIT_capacity(x_datasets, y_datasets, seq_len, hop, sample, extract_c_only=False):\n",
    "    from pandas import read_csv, DataFrame\n",
    "    V = []\n",
    "    I = []\n",
    "    T = []\n",
    "    C = []\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    SS = []\n",
    "\n",
    "    for x_data, y_data in zip(x_datasets, y_datasets):\n",
    "        # Load VIT from charging profile\n",
    "        x_df = read_csv(x_data).dropna()\n",
    "        x_df = x_df[['cycle', 'voltage_battery', 'current_battery', 'temp_battery']]\n",
    "        x_df = x_df[x_df['cycle'] != 0]  # cycle ke-0 tidak masuk\n",
    "        x_df = x_df.reset_index().drop(columns=\"index\")\n",
    "        x_len = len(x_df.cycle.unique())  # - seq_len\n",
    "\n",
    "        # Load capacity from discharging profile\n",
    "        y_df = read_csv(y_data).dropna()\n",
    "        y_df['cycle_idx'] = y_df.index + 1\n",
    "        y_df = y_df[['capacity', 'cycle_idx']]\n",
    "        y_df = y_df.values  # Convert pandas dataframe to numpy array\n",
    "        y_df = y_df.astype('float32')  # Convert values to float\n",
    "        y_len = len(y_df)  # - seq_len\n",
    "\n",
    "        data_len = np.int32(np.floor((y_len - seq_len - 1) / hop)) + 1\n",
    "\n",
    "        for i in range(y_len):\n",
    "            cy = x_df.cycle.unique()[i]\n",
    "            df = x_df.loc[x_df['cycle'] == cy]\n",
    "            # Capacity measured\n",
    "            cap = np.array([y_df[i, 0]])\n",
    "            C.append(cap)\n",
    "            df_C = DataFrame(C).values\n",
    "            scaled_C, scaler_C = preprocess(df_C)\n",
    "            scaled_C = scaled_C.astype('float32')[:, :]\n",
    "\n",
    "            le = len(df['voltage_battery']) % sample\n",
    "\n",
    "            # Voltage measured\n",
    "            vTemp = df['voltage_battery'].to_numpy()\n",
    "            if le != 0:\n",
    "                vTemp = vTemp[0:-le]\n",
    "            vTemp = np.reshape(vTemp, (len(vTemp) // sample, -1)) #, order=\"F\")\n",
    "            vTemp = vTemp.mean(axis=0)\n",
    "            V.append(vTemp)\n",
    "            df_V = DataFrame(V).values\n",
    "            scaled_V, scaler = preprocess(df_V)\n",
    "            scaled_V = scaled_V.astype('float32')[:, :]\n",
    "\n",
    "        if extract_c_only:\n",
    "            for i in range(data_len):\n",
    "                x.append(scaled_C[(hop * i):(hop * i + seq_len)])\n",
    "                y.append(scaled_C[hop * i + seq_len])\n",
    "        else:\n",
    "            for i in range(data_len):\n",
    "                x.append(scaled_V[(hop * i):(hop * i + seq_len)])\n",
    "                y.append(scaled_C[hop * i + seq_len])\n",
    "    return np.array(x), np.array(y), scaler_C\n",
    "\n",
    "\n",
    "def main():\n",
    "    pth = pr.pth\n",
    "    train_x_files = [os.path.join(pth, 'charge/train', f) for f in os.listdir(os.path.join(pth, 'charge/train'))]\n",
    "    train_x_files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    train_y_files = [os.path.join(pth, 'discharge/train', f) for f in os.listdir(os.path.join(pth, 'discharge/train'))]\n",
    "    train_y_files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    test_x_data = [os.path.join(pth, 'charge/test', f) for f in os.listdir(os.path.join(pth, 'charge/test'))]\n",
    "    test_y_data = [os.path.join(pth, 'discharge/test', f) for f in os.listdir(os.path.join(pth, 'discharge/test'))]\n",
    "    print(\"train X:\", train_x_files)\n",
    "    print(\"train Y:\", train_y_files)\n",
    "\n",
    "    folds = list(KFold(n_splits=pr.k, shuffle=True, random_state=pr.random, ).split(train_x_files))\n",
    "\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "        print('\\nFold', j + 1)\n",
    "        train_x_data = [train_x_files[train_idx[i]] for i in range(len(train_idx))]\n",
    "        train_y_data = [train_y_files[train_idx[i]] for i in range(len(train_idx))]\n",
    "        val_x_data = [train_x_files[val_idx[i]] for i in range(len(val_idx))]\n",
    "        val_y_data = [train_y_files[val_idx[i]] for i in range(len(val_idx))]\n",
    "        print(\"train X:\", train_x_data)\n",
    "        print(\"train y:\", train_y_data)\n",
    "        print(\"val X:\", val_x_data)\n",
    "        print(\"val y\", val_y_data)\n",
    "        print(\"test: x\", test_x_data)\n",
    "        print(\"test: y\", test_y_data)\n",
    "\n",
    "        # lstm data\n",
    "        trainX_lstm, trainY_lstm, SS_tr_lstm = extract_VIT_capacity(train_x_data, train_y_data, pr.seq_len_lstm, pr.hop, pr.sample,\n",
    "                                                                          extract_c_only=True)\n",
    "        valX_lstm, valY_lstm, SS_val_lstm = extract_VIT_capacity(val_x_data, val_y_data, pr.seq_len_lstm, pr.hop, pr.sample,\n",
    "                                                                       extract_c_only=True)\n",
    "        testX_lstm, testY_lstm, SS_tt_lstm = extract_VIT_capacity(test_x_data, test_y_data, pr.seq_len_lstm, pr.hop, pr.sample,\n",
    "                                                                        extract_c_only=True)\n",
    "        print('Input shape: {}'.format(trainX_lstm.shape))\n",
    "\n",
    "        # CNN data\n",
    "        trainX_cnn, trainY_cnn, SS_tr_cnn = extract_VIT_capacity(train_x_data, train_y_data, pr.seq_len_cnn, pr.hop, pr.sample)\n",
    "        valX_cnn, valY_cnn, SS_val_cnn = extract_VIT_capacity(val_x_data, val_y_data, pr.seq_len_cnn, pr.hop, pr.sample)\n",
    "        testX_cnn, testY_cnn, SS_tt_cnn = extract_VIT_capacity(test_x_data, test_y_data, pr.seq_len_cnn, pr.hop, pr.sample)\n",
    "        print('Input shape: {}'.format(trainX_cnn.shape))\n",
    "\n",
    "        # define inputs\n",
    "        input_CNN = Input(shape=(pr.seq_len_cnn, trainX_cnn.shape[-1]), name=\"CNN_Input\")\n",
    "        input_LSTM = Input(shape=(pr.seq_len_lstm, trainX_lstm.shape[-1]), name=\"LSTM_Input\")\n",
    "\n",
    "        LSTM_layer = LSTM(32, activation='tanh', return_sequences=True, name=\"LSTM_layer\")(input_LSTM)\n",
    "\n",
    "        CNN_layer = Conv1D(32, 5, activation='relu', strides=1, padding=\"same\", name=\"CNN_layer\")(input_CNN)\n",
    "\n",
    "        concat = concatenate([LSTM_layer, CNN_layer])\n",
    "\n",
    "        flat = Flatten()(concat)\n",
    "\n",
    "        output = Dense(32, activation='relu', name=\"predictor\")(flat)\n",
    "        output = Dense(1, name=\"Output\")(output)\n",
    "\n",
    "        model = Model(inputs=[input_LSTM, input_CNN], outputs=[output])\n",
    "\n",
    "        optim = Adam(learning_rate=0.001)\n",
    "        model.compile(loss='mse', optimizer=optim)\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(x=[trainX_lstm, trainX_cnn],\n",
    "                            y=[trainY_lstm, trainY_cnn],\n",
    "                            validation_data=([valX_lstm, valX_cnn],\n",
    "                                             [valY_lstm, valY_cnn]),\n",
    "                            batch_size=50,\n",
    "                            epochs=100)\n",
    "\n",
    "        save_dir = pr.save_dir\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_dir = pr.model_dir + '_k' + str(j + 1)\n",
    "        if not os.path.exists(os.path.join(save_dir, model_dir)):\n",
    "            os.makedirs(os.path.join(save_dir, model_dir))\n",
    "\n",
    "        model.save(save_dir + model_dir + \"/saved_model_and_weight\")\n",
    "        print(\"bobot dan model tersimpan\")\n",
    "\n",
    "        val_loss = []\n",
    "        val_results = model.evaluate([valX_lstm, valX_cnn],\n",
    "                                     [valY_lstm, valY_cnn])\n",
    "        val_loss.append(val_results)\n",
    "        print('Val loss:', val_results)\n",
    "\n",
    "        test_loss = []\n",
    "        results = model.evaluate([testX_lstm, testX_cnn],\n",
    "                                 [testY_lstm, testY_cnn])\n",
    "        test_loss.append(results)\n",
    "        print('Test loss:', results)\n",
    "\n",
    "        valPredict = model.predict([valX_lstm, valX_cnn])\n",
    "        testPredict = model.predict([testX_lstm, testX_cnn])\n",
    "\n",
    "        inv_valY = SS_val_cnn.inverse_transform(valY_cnn)\n",
    "        inv_valPredict = SS_val_cnn.inverse_transform(valPredict)\n",
    "\n",
    "        inv_testY = SS_tt_cnn.inverse_transform(testY_cnn)\n",
    "        inv_testPredict = SS_tt_cnn.inverse_transform(testPredict)\n",
    "\n",
    "        test_mae = mean_absolute_error(inv_testY, inv_testPredict)\n",
    "        test_mse = mean_squared_error(inv_testY, inv_testPredict)\n",
    "        test_mape = mean_absolute_percentage_error(inv_testY, inv_testPredict)\n",
    "        test_rmse = np.sqrt(mean_squared_error(inv_testY, inv_testPredict))\n",
    "        print('\\nTest Mean Absolute Error: %f MAE' % test_mae)\n",
    "        print('Test Mean Square Error: %f MSE' % test_mse)\n",
    "        print('Test Mean Absolute Percentage Error: %f MAPE' % test_mape)\n",
    "        print('Test Root Mean Squared Error: %f RMSE' % test_rmse)\n",
    "\n",
    "        with open(os.path.join(save_dir, model_dir, 'eval_metrics.txt'), 'w') as f:\n",
    "            f.write('Train data: ')\n",
    "            f.write(json.dumps(train_x_data))\n",
    "            f.write('\\nVal data: ')\n",
    "            f.write(json.dumps(val_x_data))\n",
    "            f.write('\\nTest data: ')\n",
    "            f.write(json.dumps(test_x_data))\n",
    "            f.write('\\n\\nTest Mean Absolute Error: ')\n",
    "            f.write(json.dumps(str(test_mae)))\n",
    "            f.write('\\nTest Mean Square Error: ')\n",
    "            f.write(json.dumps(str(test_mse)))\n",
    "            f.write('\\nTest Mean Absolute Percentage Error: ')\n",
    "            f.write(json.dumps(str(test_mape)))\n",
    "            f.write('\\nTest Root Mean Squared Error: ')\n",
    "            f.write(json.dumps(str(test_rmse)))\n",
    "\n",
    "        # Save test prediction to text file\n",
    "        testPred_file = open(os.path.join(save_dir, model_dir, 'test_predict.txt'), 'w')\n",
    "        for row in inv_testPredict:\n",
    "            np.savetxt(testPred_file, row)\n",
    "        testPred_file.close()\n",
    "\n",
    "        testY_file = open(os.path.join(save_dir, model_dir, 'test_true.txt'), 'w')\n",
    "        for row in inv_testY:\n",
    "            np.savetxt(testY_file, row)\n",
    "        testY_file.close()\n",
    "\n",
    "        # plot graph\n",
    "        utils.plot_loss(history, save_dir, model_dir)\n",
    "        utils.plot_pred(inv_valPredict, inv_valY, save_dir, model_dir, \"val_pred\")\n",
    "        utils.plot_pred(inv_testPredict, inv_testY, save_dir, model_dir, \"test_pred\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
